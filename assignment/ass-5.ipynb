{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f10a722-a37d-46fc-bb54-721665e5cf39",
   "metadata": {},
   "source": [
    "**Text Classification using BERT Embeddings and XGBoost for Performance Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "191f7044-32db-467a-83ab-61f06dcd3752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batches: 16\n",
      "Processing batch 1/16...\n",
      "Processing batch 2/16...\n",
      "Processing batch 3/16...\n",
      "Processing batch 4/16...\n",
      "Processing batch 5/16...\n",
      "Processing batch 6/16...\n",
      "Processing batch 7/16...\n",
      "Processing batch 8/16...\n",
      "Processing batch 9/16...\n",
      "Processing batch 10/16...\n",
      "Processing batch 11/16...\n",
      "Processing batch 12/16...\n",
      "Processing batch 13/16...\n",
      "Processing batch 14/16...\n",
      "Processing batch 15/16...\n",
      "Processing batch 16/16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:14:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "ROC AUC: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Load your dataset\n",
    "data_path = 'updated_dataset.csv'  # Replace with your dataset file path\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Update these column names to match your dataset\n",
    "text_column = 'Transcript'  # Column containing textual data\n",
    "label_column = 'Performance (select/reject)'  # Column containing labels\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Function to get BERT embeddings in batches\n",
    "def get_bert_embeddings_batch(texts, tokenizer, model, batch_size=32):\n",
    "    embeddings = []\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    print(f\"Total Batches: {total_batches}\")\n",
    "    \n",
    "    for i in range(total_batches):\n",
    "        print(f\"Processing batch {i + 1}/{total_batches}...\")\n",
    "        batch = texts[i * batch_size:(i + 1) * batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Generate BERT embeddings\n",
    "df['bert_embeddings'] = get_bert_embeddings_batch(df[text_column].astype(str).tolist(), tokenizer, model)\n",
    "\n",
    "# Expand embeddings into separate columns\n",
    "embeddings_expanded = pd.DataFrame(df['bert_embeddings'].tolist(), index=df.index)\n",
    "embeddings_expanded.columns = [f'emb_{i}' for i in range(embeddings_expanded.shape[1])]\n",
    "\n",
    "# Combine original data and embeddings\n",
    "df_expanded = pd.concat([df, embeddings_expanded], axis=1)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_expanded['label_encoded'] = label_encoder.fit_transform(df_expanded[label_column])\n",
    "\n",
    "# Select features and target\n",
    "embedding_columns = [col for col in df_expanded.columns if col.startswith('emb_')]\n",
    "X = df_expanded[embedding_columns]\n",
    "y = df_expanded['label_encoded']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train XGBoost classifier\n",
    "model = XGBClassifier(use_label_encoder=False, max_depth=5, n_estimators=200, learning_rate=0.1, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ac6be-c353-4ddc-93a9-02fb63b4fb9b",
   "metadata": {},
   "source": [
    "1.Text Preprocessing: Text data is transformed into numerical BERT embeddings using a pre-trained DistilBERT model.\n",
    "\n",
    "2.Feature Engineering: The embeddings are expanded into separate columns for use in machine learning models.\n",
    "\n",
    "3.Modeling: XGBoost classifier is employed to predict the performance of candidates based on text features.\n",
    "\n",
    "4.Evaluation: Accuracy and ROC AUC score are used to assess the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c66d6f-f0b7-4499-90dd-cc088c566ab1",
   "metadata": {},
   "source": [
    "**Text Classification with BERT Embeddings and a Custom Neural Network Using PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74174c9c-70d2-4474-8d36-1d5c17d4604c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batches: 16\n",
      "Processing batch 1/16...\n",
      "Processing batch 2/16...\n",
      "Processing batch 3/16...\n",
      "Processing batch 4/16...\n",
      "Processing batch 5/16...\n",
      "Processing batch 6/16...\n",
      "Processing batch 7/16...\n",
      "Processing batch 8/16...\n",
      "Processing batch 9/16...\n",
      "Processing batch 10/16...\n",
      "Processing batch 11/16...\n",
      "Processing batch 12/16...\n",
      "Processing batch 13/16...\n",
      "Processing batch 14/16...\n",
      "Processing batch 15/16...\n",
      "Processing batch 16/16...\n",
      "Epoch 1/10, Loss: 0.6535\n",
      "Epoch 2/10, Loss: 0.5094\n",
      "Epoch 3/10, Loss: 0.3828\n",
      "Epoch 4/10, Loss: 0.3350\n",
      "Epoch 5/10, Loss: 0.3253\n",
      "Epoch 6/10, Loss: 0.3210\n",
      "Epoch 7/10, Loss: 0.3190\n",
      "Epoch 8/10, Loss: 0.3186\n",
      "Epoch 9/10, Loss: 0.3170\n",
      "Epoch 10/10, Loss: 0.3164\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "data_path = 'updated_dataset.csv'  # Replace with your dataset file path\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Update these column names to match your dataset\n",
    "text_column = 'Transcript'  # Column containing textual data\n",
    "label_column = 'Performance (select/reject)'  # Column containing labels\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Function to get BERT embeddings in batches\n",
    "def get_bert_embeddings_batch(texts, tokenizer, model, batch_size=32):\n",
    "    embeddings = []\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    print(f\"Total Batches: {total_batches}\")\n",
    "    \n",
    "    for i in range(total_batches):\n",
    "        print(f\"Processing batch {i + 1}/{total_batches}...\")\n",
    "        batch = texts[i * batch_size:(i + 1) * batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Generate BERT embeddings\n",
    "df['bert_embeddings'] = get_bert_embeddings_batch(df[text_column].astype(str).tolist(), tokenizer, model)\n",
    "\n",
    "# Expand embeddings into separate columns\n",
    "embeddings_expanded = pd.DataFrame(df['bert_embeddings'].tolist(), index=df.index)\n",
    "embeddings_expanded.columns = [f'emb_{i}' for i in range(embeddings_expanded.shape[1])]\n",
    "\n",
    "# Combine original data and embeddings\n",
    "df_expanded = pd.concat([df, embeddings_expanded], axis=1)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_expanded['label_encoded'] = label_encoder.fit_transform(df_expanded[label_column])\n",
    "\n",
    "# Select features and target\n",
    "embedding_columns = [col for col in df_expanded.columns if col.startswith('emb_')]\n",
    "X = df_expanded[embedding_columns].values\n",
    "y = df_expanded['label_encoded'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create PyTorch Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define Neural Network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "input_size = len(embedding_columns)\n",
    "num_classes = len(np.unique(y))\n",
    "model = NeuralNetwork(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the Model\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for features, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Evaluate the Model\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for features, labels in dataloader:\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.numpy())\n",
    "            actuals.extend(labels.numpy())\n",
    "    return predictions, actuals\n",
    "\n",
    "predictions, actuals = evaluate_model(model, test_loader)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = np.mean(np.array(predictions) == np.array(actuals))\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b171604-95ee-42eb-86ed-694d6574f6c3",
   "metadata": {},
   "source": [
    "1.Text Embedding: The BERT model is used to generate embeddings from text, which are then used as features for classification.\n",
    "\n",
    "2.Custom Neural Network: A simple neural network with two hidden layers is defined to classify the text data based on BERT embeddings.\n",
    "\n",
    "3.DataLoader: PyTorch's DataLoader is used to handle the dataset in batches for training and evaluation.\n",
    "\n",
    "4.Training and Evaluation: The model is trained for 10 epochs and evaluated using accuracy as the performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84303f3a-5611-4296-8f0a-86fafc482e42",
   "metadata": {},
   "source": [
    "**Comparing Multiple Classifiers for Text Classification Using BERT Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8375d3f-e064-474c-b1da-954f40cfdc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anaconda3\\lib\\site-packages (4.48.0)\n",
      "Requirement already satisfied: torch in d:\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: xgboost in d:\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.2.7-cp312-cp312-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in d:\\anaconda3\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in d:\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Collecting graphviz (from catboost)\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda3\\lib\\site-packages (from catboost) (3.9.2)\n",
      "Requirement already satisfied: plotly in d:\\anaconda3\\lib\\site-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in d:\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in d:\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in d:\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading catboost-1.2.7-cp312-cp312-win_amd64.whl (101.7 MB)\n",
      "   ---------------------------------------- 0.0/101.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/101.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/101.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/101.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/101.7 MB 645.7 kB/s eta 0:02:37\n",
      "   ---------------------------------------- 0.8/101.7 MB 763.2 kB/s eta 0:02:13\n",
      "   ---------------------------------------- 1.0/101.7 MB 825.2 kB/s eta 0:02:02\n",
      "    --------------------------------------- 1.3/101.7 MB 919.8 kB/s eta 0:01:50\n",
      "    --------------------------------------- 1.6/101.7 MB 987.0 kB/s eta 0:01:42\n",
      "    --------------------------------------- 1.8/101.7 MB 1.0 MB/s eta 0:01:36\n",
      "    --------------------------------------- 2.4/101.7 MB 1.2 MB/s eta 0:01:26\n",
      "   - -------------------------------------- 2.6/101.7 MB 1.2 MB/s eta 0:01:22\n",
      "   - -------------------------------------- 2.9/101.7 MB 1.2 MB/s eta 0:01:21\n",
      "   - -------------------------------------- 3.4/101.7 MB 1.3 MB/s eta 0:01:16\n",
      "   - -------------------------------------- 3.9/101.7 MB 1.4 MB/s eta 0:01:12\n",
      "   - -------------------------------------- 4.2/101.7 MB 1.4 MB/s eta 0:01:10\n",
      "   - -------------------------------------- 4.7/101.7 MB 1.4 MB/s eta 0:01:07\n",
      "   - -------------------------------------- 5.0/101.7 MB 1.5 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 5.5/101.7 MB 1.5 MB/s eta 0:01:04\n",
      "   -- ------------------------------------- 6.0/101.7 MB 1.5 MB/s eta 0:01:02\n",
      "   -- ------------------------------------- 6.3/101.7 MB 1.6 MB/s eta 0:01:02\n",
      "   -- ------------------------------------- 6.8/101.7 MB 1.6 MB/s eta 0:01:01\n",
      "   -- ------------------------------------- 7.1/101.7 MB 1.6 MB/s eta 0:01:01\n",
      "   -- ------------------------------------- 7.3/101.7 MB 1.6 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 7.9/101.7 MB 1.6 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.4/101.7 MB 1.6 MB/s eta 0:00:58\n",
      "   --- ------------------------------------ 9.2/101.7 MB 1.7 MB/s eta 0:00:55\n",
      "   --- ------------------------------------ 9.7/101.7 MB 1.7 MB/s eta 0:00:53\n",
      "   ---- ----------------------------------- 10.5/101.7 MB 1.8 MB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 11.0/101.7 MB 1.8 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 11.5/101.7 MB 1.9 MB/s eta 0:00:49\n",
      "   ---- ----------------------------------- 12.1/101.7 MB 1.9 MB/s eta 0:00:48\n",
      "   ----- ---------------------------------- 12.8/101.7 MB 1.9 MB/s eta 0:00:46\n",
      "   ----- ---------------------------------- 13.6/101.7 MB 2.0 MB/s eta 0:00:45\n",
      "   ----- ---------------------------------- 14.4/101.7 MB 2.0 MB/s eta 0:00:43\n",
      "   ----- ---------------------------------- 15.2/101.7 MB 2.1 MB/s eta 0:00:42\n",
      "   ------ --------------------------------- 16.0/101.7 MB 2.1 MB/s eta 0:00:41\n",
      "   ------ --------------------------------- 16.8/101.7 MB 2.2 MB/s eta 0:00:39\n",
      "   ------ --------------------------------- 17.6/101.7 MB 2.2 MB/s eta 0:00:38\n",
      "   ------- -------------------------------- 18.4/101.7 MB 2.3 MB/s eta 0:00:37\n",
      "   ------- -------------------------------- 19.1/101.7 MB 2.3 MB/s eta 0:00:36\n",
      "   ------- -------------------------------- 19.9/101.7 MB 2.3 MB/s eta 0:00:35\n",
      "   -------- ------------------------------- 20.4/101.7 MB 2.4 MB/s eta 0:00:35\n",
      "   -------- ------------------------------- 21.2/101.7 MB 2.4 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 21.8/101.7 MB 2.4 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 22.3/101.7 MB 2.4 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 22.8/101.7 MB 2.4 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 23.3/101.7 MB 2.4 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 23.9/101.7 MB 2.4 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 24.1/101.7 MB 2.4 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 24.4/101.7 MB 2.4 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 25.2/101.7 MB 2.4 MB/s eta 0:00:33\n",
      "   ---------- ----------------------------- 25.7/101.7 MB 2.4 MB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 26.2/101.7 MB 2.4 MB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 26.7/101.7 MB 2.4 MB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 27.0/101.7 MB 2.4 MB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 27.5/101.7 MB 2.4 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 28.0/101.7 MB 2.4 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 28.3/101.7 MB 2.4 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 28.8/101.7 MB 2.4 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 29.6/101.7 MB 2.4 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 30.1/101.7 MB 2.4 MB/s eta 0:00:30\n",
      "   ------------ --------------------------- 30.9/101.7 MB 2.4 MB/s eta 0:00:30\n",
      "   ------------ --------------------------- 31.5/101.7 MB 2.4 MB/s eta 0:00:30\n",
      "   ------------ --------------------------- 32.2/101.7 MB 2.4 MB/s eta 0:00:29\n",
      "   ------------ --------------------------- 33.0/101.7 MB 2.4 MB/s eta 0:00:29\n",
      "   ------------- -------------------------- 33.6/101.7 MB 2.4 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 34.3/101.7 MB 2.5 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 35.4/101.7 MB 2.5 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 36.2/101.7 MB 2.5 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 36.4/101.7 MB 2.5 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 37.0/101.7 MB 2.5 MB/s eta 0:00:26\n",
      "   -------------- ------------------------- 37.2/101.7 MB 2.5 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 37.7/101.7 MB 2.5 MB/s eta 0:00:26\n",
      "   -------------- ------------------------- 38.0/101.7 MB 2.5 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 38.3/101.7 MB 2.4 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 38.8/101.7 MB 2.4 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 39.3/101.7 MB 2.4 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 39.6/101.7 MB 2.5 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 40.1/101.7 MB 2.4 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 40.6/101.7 MB 2.4 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 41.7/101.7 MB 2.5 MB/s eta 0:00:25\n",
      "   ---------------- ----------------------- 42.7/101.7 MB 2.5 MB/s eta 0:00:24\n",
      "   ----------------- ---------------------- 43.3/101.7 MB 2.5 MB/s eta 0:00:24\n",
      "   ----------------- ---------------------- 44.0/101.7 MB 2.5 MB/s eta 0:00:24\n",
      "   ----------------- ---------------------- 45.1/101.7 MB 2.5 MB/s eta 0:00:23\n",
      "   ------------------ --------------------- 45.9/101.7 MB 2.6 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 46.7/101.7 MB 2.6 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 47.4/101.7 MB 2.6 MB/s eta 0:00:21\n",
      "   ------------------ --------------------- 48.0/101.7 MB 2.6 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 48.5/101.7 MB 2.6 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 49.3/101.7 MB 2.6 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 49.8/101.7 MB 2.6 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 50.6/101.7 MB 2.6 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 51.6/101.7 MB 2.6 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 52.2/101.7 MB 2.6 MB/s eta 0:00:19\n",
      "   -------------------- ------------------- 53.0/101.7 MB 2.6 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 54.0/101.7 MB 2.7 MB/s eta 0:00:18\n",
      "   --------------------- ------------------ 55.1/101.7 MB 2.7 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 56.4/101.7 MB 2.7 MB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 56.6/101.7 MB 2.7 MB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 57.1/101.7 MB 2.7 MB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 57.7/101.7 MB 2.7 MB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 58.2/101.7 MB 2.7 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 59.0/101.7 MB 2.7 MB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 60.0/101.7 MB 2.7 MB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 60.8/101.7 MB 2.7 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 61.6/101.7 MB 2.8 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 62.7/101.7 MB 2.8 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 63.7/101.7 MB 2.8 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 64.7/101.7 MB 2.8 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 66.1/101.7 MB 2.8 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 66.8/101.7 MB 2.9 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 68.4/101.7 MB 2.9 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 69.2/101.7 MB 2.9 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 70.8/101.7 MB 2.9 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 72.1/101.7 MB 3.0 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 73.4/101.7 MB 3.0 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 74.2/101.7 MB 3.0 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 75.2/101.7 MB 3.0 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 76.0/101.7 MB 3.0 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 77.3/101.7 MB 3.1 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 78.6/101.7 MB 3.1 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 80.0/101.7 MB 3.1 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 81.5/101.7 MB 3.1 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 82.6/101.7 MB 3.2 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 83.9/101.7 MB 3.2 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 85.2/101.7 MB 3.2 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 86.5/101.7 MB 3.2 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 87.8/101.7 MB 3.2 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 89.1/101.7 MB 3.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 90.2/101.7 MB 3.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 91.5/101.7 MB 3.3 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 92.5/101.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 93.3/101.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 94.4/101.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 95.7/101.7 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 96.7/101.7 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 97.8/101.7 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 99.1/101.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  100.1/101.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  101.4/101.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  101.7/101.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  101.7/101.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  101.7/101.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  101.7/101.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  101.7/101.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.7/101.7 MB 3.4 MB/s eta 0:00:00\n",
      "Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz, catboost\n",
      "Successfully installed catboost-1.2.7 graphviz-0.20.3\n",
      "Total Batches: 16\n",
      "Processing batch 1/16...\n",
      "Processing batch 2/16...\n",
      "Processing batch 3/16...\n",
      "Processing batch 4/16...\n",
      "Processing batch 5/16...\n",
      "Processing batch 6/16...\n",
      "Processing batch 7/16...\n",
      "Processing batch 8/16...\n",
      "Processing batch 9/16...\n",
      "Processing batch 10/16...\n",
      "Processing batch 11/16...\n",
      "Processing batch 12/16...\n",
      "Processing batch 13/16...\n",
      "Processing batch 14/16...\n",
      "Processing batch 15/16...\n",
      "Processing batch 16/16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:51:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[XGBoost Results]\n",
      "Accuracy: 0.98\n",
      "ROC AUC: 0.98\n",
      "\n",
      "[CatBoost Results]\n",
      "Accuracy: 0.99\n",
      "ROC AUC: 0.99\n",
      "\n",
      "[RandomForest Results]\n",
      "Accuracy: 1.0\n",
      "ROC AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers torch scikit-learn xgboost pandas catboost\n",
    "\n",
    "# Imports\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Load your dataset\n",
    "data_path = 'updated_dataset.csv'  # Replace with your dataset file path\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Update these column names to match your dataset\n",
    "text_column = 'Transcript'  # Column containing textual data\n",
    "label_column = 'Performance (select/reject)'  # Column containing labels\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Function to get BERT embeddings in batches\n",
    "def get_bert_embeddings_batch(texts, tokenizer, model, batch_size=32):\n",
    "    embeddings = []\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    print(f\"Total Batches: {total_batches}\")\n",
    "    \n",
    "    for i in range(total_batches):\n",
    "        print(f\"Processing batch {i + 1}/{total_batches}...\")\n",
    "        batch = texts[i * batch_size:(i + 1) * batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Generate BERT embeddings\n",
    "df['bert_embeddings'] = get_bert_embeddings_batch(df[text_column].astype(str).tolist(), tokenizer, model)\n",
    "\n",
    "# Expand embeddings into separate columns\n",
    "embeddings_expanded = pd.DataFrame(df['bert_embeddings'].tolist(), index=df.index)\n",
    "embeddings_expanded.columns = [f'emb_{i}' for i in range(embeddings_expanded.shape[1])]\n",
    "\n",
    "# Combine original data and embeddings\n",
    "df_expanded = pd.concat([df, embeddings_expanded], axis=1)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_expanded['label_encoded'] = label_encoder.fit_transform(df_expanded[label_column])\n",
    "\n",
    "# Select features and target\n",
    "embedding_columns = [col for col in df_expanded.columns if col.startswith('emb_')]\n",
    "X = df_expanded[embedding_columns]\n",
    "y = df_expanded['label_encoded']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 1. XGBoost Classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, max_depth=5, n_estimators=200, learning_rate=0.1, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "print(\"\\n[XGBoost Results]\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, xgb_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, xgb_pred))\n",
    "\n",
    "# 2. CatBoost Classifier\n",
    "catboost_model = CatBoostClassifier(depth=6, learning_rate=0.1, iterations=200, verbose=0)\n",
    "catboost_model.fit(X_train, y_train)\n",
    "catboost_pred = catboost_model.predict(X_test)\n",
    "print(\"\\n[CatBoost Results]\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, catboost_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, catboost_pred))\n",
    "\n",
    "# 3. RandomForest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "print(\"\\n[RandomForest Results]\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, rf_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e31648-cec6-4b97-874d-f5fb4b2bd2f3",
   "metadata": {},
   "source": [
    "1.Text Embedding: BERT is used to generate embeddings for the text data to capture meaningful information for classification.\n",
    "\n",
    "2Multiple Classifiers: XGBoost, CatBoost, and RandomForest are used for classification to compare their performance.\n",
    "\n",
    "3.Model Evaluation: Accuracy and ROC AUC scores are calculated to evaluate the models' performance.\n",
    "\n",
    "4.Comparing Models: The results help determine which classifier performs the best for the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a8fb49-91fa-4d98-ad97-9d4caaae1031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
